{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "245f7718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.22 requires botocore==1.29.22, but you have botocore 1.29.23 which is incompatible.\n",
      "aiobotocore 2.0.1 requires botocore<1.22.9,>=1.22.8, but you have botocore 1.29.23 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q \"sagemaker>=2.70.0\" \"transformers==4.11.0\" --upgrade\n",
    "\n",
    "!pip install -q \"datasets==1.13\" --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3da0636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\r\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2baf24cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "212820c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bfeb3d",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0292c235",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(subset=\"all\", shuffle=True, remove=(\"headers\", \"footers\", \"quotes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8015cc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = dataset.data\n",
    "labels = dataset.target\n",
    "df=pd.DataFrame({\"documents\":documents,\"labels\":labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "174c18ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18841</th>\n",
       "      <td>DN&gt; From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18842</th>\n",
       "      <td>\\nNot in isolated ground recepticles (usually ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18843</th>\n",
       "      <td>I just installed a DX2-66 CPU in a clone mothe...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18844</th>\n",
       "      <td>\\nWouldn't this require a hyper-sphere.  In 3-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18845</th>\n",
       "      <td>After a tip from Gary Crum (crum@fcom.cc.utah....</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18846 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               documents  labels\n",
       "0      \\n\\nI am sure some bashers of Pens fans are pr...      10\n",
       "1      My brother is in the market for a high-perform...       3\n",
       "2      \\n\\n\\n\\n\\tFinally you said what you dream abou...      17\n",
       "3      \\nThink!\\n\\nIt's the SCSI card doing the DMA t...       3\n",
       "4      1)    I have an old Jasmine drive which I cann...       4\n",
       "...                                                  ...     ...\n",
       "18841  DN> From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...      13\n",
       "18842  \\nNot in isolated ground recepticles (usually ...      12\n",
       "18843  I just installed a DX2-66 CPU in a clone mothe...       3\n",
       "18844  \\nWouldn't this require a hyper-sphere.  In 3-...       1\n",
       "18845  After a tip from Gary Crum (crum@fcom.cc.utah....       7\n",
       "\n",
       "[18846 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23dc78ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,string\n",
    "\n",
    "def text_cleaner(text):\n",
    "\n",
    "    '''some text cleaning method'''\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "\n",
    "    text = re.sub('\\n', '', text)\n",
    "\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46547d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"documents\"]=df[\"documents\"].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7a0593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.head(1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad01129e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1100, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1815c873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am sure some bashers of pens fans are pretty...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my brother is in the market for a highperforma...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\tfinally you said what you dream about medite...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thinkits the scsi card doing the dma transfers...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i have an old jasmine drive which i cannot...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>i am hoping to produce the first update of the...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>in article  manesmagpielinknetcom steve     i...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>sorry malcolm but i rather believe jesus than...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>i have placed a new release of my axe editor i...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>all children are born pure ie without sinhowev...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              documents  labels\n",
       "0     i am sure some bashers of pens fans are pretty...      10\n",
       "1     my brother is in the market for a highperforma...       3\n",
       "2     \\tfinally you said what you dream about medite...      17\n",
       "3     thinkits the scsi card doing the dma transfers...       3\n",
       "4         i have an old jasmine drive which i cannot...       4\n",
       "...                                                 ...     ...\n",
       "1095  i am hoping to produce the first update of the...       9\n",
       "1096   in article  manesmagpielinknetcom steve     i...      16\n",
       "1097   sorry malcolm but i rather believe jesus than...      19\n",
       "1098  i have placed a new release of my axe editor i...       5\n",
       "1099  all children are born pure ie without sinhowev...      15\n",
       "\n",
       "[1100 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0f5b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ae4e9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>anyone familiar with this video card what chi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>yeah it might if you only read the part you qu...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>im no defender of the aec but it is worth noti...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>phil didnt one of the early jet fighters have ...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>the problem is that the pins in the adb connec...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>would you believe that there is a letter i...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>\\tim not sure that you can distinguish between...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>some of the davidians are blacknext question</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>ok as one last attempt ill take a different ta...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              documents  labels\n",
       "456    anyone familiar with this video card what chi...       3\n",
       "1039  yeah it might if you only read the part you qu...      13\n",
       "1036  im no defender of the aec but it is worth noti...      18\n",
       "597                                                           8\n",
       "75    phil didnt one of the early jet fighters have ...      14\n",
       "...                                                 ...     ...\n",
       "447   the problem is that the pins in the adb connec...       4\n",
       "36        would you believe that there is a letter i...       4\n",
       "479   \\tim not sure that you can distinguish between...      19\n",
       "591        some of the davidians are blacknext question      16\n",
       "937   ok as one last attempt ill take a different ta...       8\n",
       "\n",
       "[1100 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5418f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = df.shape[0]\n",
    "train = int(.8* rows)\n",
    "test = rows-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4621fd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((880, 2), (220, 2))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:train].shape,df[train:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7fec466",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:train].to_csv('train.csv'\n",
    "                          ,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdcac803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Validation Set\n",
    "df[train:].to_csv('test.csv'\n",
    "                          ,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad0f271b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>anyone familiar with this video card what chi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>yeah it might if you only read the part you qu...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>im no defender of the aec but it is worth noti...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>phil didnt one of the early jet fighters have ...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>the problem is that the pins in the adb connec...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>would you believe that there is a letter i...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>\\tim not sure that you can distinguish between...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>some of the davidians are blacknext question</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>ok as one last attempt ill take a different ta...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              documents  labels\n",
       "456    anyone familiar with this video card what chi...       3\n",
       "1039  yeah it might if you only read the part you qu...      13\n",
       "1036  im no defender of the aec but it is worth noti...      18\n",
       "597                                                           8\n",
       "75    phil didnt one of the early jet fighters have ...      14\n",
       "...                                                 ...     ...\n",
       "447   the problem is that the pins in the adb connec...       4\n",
       "36        would you believe that there is a letter i...       4\n",
       "479   \\tim not sure that you can distinguish between...      19\n",
       "591        some of the davidians are blacknext question      16\n",
       "937   ok as one last attempt ill take a different ta...       8\n",
       "\n",
       "[1100 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57e7f436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      labels\n",
       "456        3\n",
       "1039      13\n",
       "1036      18\n",
       "597        8\n",
       "75        14\n",
       "...      ...\n",
       "447        4\n",
       "36         4\n",
       "479       19\n",
       "591       16\n",
       "937        8\n",
       "\n",
       "[1100 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"labels\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1638b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64551804",
   "metadata": {},
   "source": [
    "# Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ce82d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "# from sagemaker.sklearn.estimator import SKLearn\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "import os\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aed85a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-east-2'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31f3f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'agtimeseries'\n",
    "\n",
    "training_folder = r'mce-folder/train'\n",
    "test_folder = r'mceolder/test'\n",
    "model_folder1 = r'mce-hf-folder/model1/'\n",
    "script_code_folder1=r\"mce-folder/scriptcode1\"\n",
    "\n",
    "model_folder2 = r'mce-folder/model2/'\n",
    "script_code_folder2=r\"mce-folder/scriptcode2\"\n",
    "\n",
    "training_data_loc = r's3://' + bucket_name + r'/' + training_folder\n",
    "testing_data_loc = r's3://' + bucket_name + r'/' + test_folder\n",
    "model_data_loc1 = r's3://' + bucket_name + r'/' + model_folder1\n",
    "script_code_loc1=r's3://' + bucket_name + r'/' + script_code_folder1\n",
    "\n",
    "\n",
    "model_data_loc2 = r's3://' + bucket_name + r'/' + model_folder2\n",
    "script_code_loc2=r's3://' + bucket_name + r'/' + script_code_folder2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5fa56ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://agtimeseries/mce-folder/train'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e06018ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file=\"train.csv\"\n",
    "test_file=\"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adf6c6ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://agtimeseries/mce-folder/train/train.csv'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_session.upload_data(train_file,\n",
    "                              bucket=bucket_name, \n",
    "                              key_prefix=training_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c66c9253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://agtimeseries/mceolder/test/test.csv'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_session.upload_data(test_file, \n",
    "                              bucket=bucket_name, \n",
    "                              key_prefix=test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33c00a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://agtimeseries/mce-hf-folder/model1/'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data_loc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e631f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace,TrainingCompilerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d11057ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler_config=TrainingCompilerConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a217855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25a26910",
   "metadata": {},
   "source": [
    "# HuggingFace Estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d070dbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={'epochs': 1,                                    # number of training epochs\n",
    "                 'train_batch_size': 8,                         # batch size for training\n",
    "                 'eval_batch_size': 8,                          #batch size for testing\n",
    "                 \"model_id\" :'bert-base-uncased'                #pretrained model name from hf\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "106cefa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator1 = HuggingFace(\n",
    "    entry_point          = 'script.py',       # script code to run the model      \n",
    "    source_dir           = 'Hfcode/',\n",
    "    instance_type        = 'ml.g4dn.xlarge',    # instances type used for the training job\n",
    "    instance_count       = 1,                  # the number of instances used for training\n",
    "    base_job_name        = \"bert-model\",       # the name of the training job     \n",
    "    role                 = role,               # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    transformers_version = '4.11.0',           # the transformers version used in the training job\n",
    "    pytorch_version      = '1.9.0',            # the pytorch_version version used in the training job\n",
    "    py_version           = 'py38',             # the python version used in the training job\n",
    "    compiler_config      = compiler_config,    # the compiler configuration used in the training job\n",
    "    hyperparameters      = hyperparameters,    # the hyperparameter used for running the training job\n",
    "    disable_profiler     = True,               # whether to disable the profiler during training used to gain maximum performance\n",
    "    debugger_hook_config = False,              # whether to enable the debugger hook during training used to gain maximum performance\n",
    "    output_path          = model_data_loc1,     # s3 location where the final fine-tuned model will be saved \n",
    "    code_location        = script_code_loc1     # s3 location where the estimator will push the script code\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff2f6076",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'training': training_data_loc, 'testing': testing_data_loc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e152dfa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-06 05:42:55 Starting - Starting the training job...\n",
      "2022-12-06 05:43:10 Starting - Preparing the instances for training......\n",
      "2022-12-06 05:44:11 Downloading - Downloading input data...\n",
      "2022-12-06 05:44:31 Training - Downloading the training image..............................\n",
      "2022-12-06 05:49:38 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-12-06 05:49:50,783 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-12-06 05:49:50,808 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-12-06 05:49:50,812 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-12-06 05:49:51,079 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_training_compiler_debug_mode\": false,\n",
      "        \"sagemaker_training_compiler_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"eval_batch_size\": 8,\n",
      "        \"model_id\": \"bert-base-uncased\",\n",
      "        \"train_batch_size\": 8\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"bert-model-2022-12-06-05-42-55-321\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://agtimeseries/mce-folder/scriptcode1/bert-model-2022-12-06-05-42-55-321/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"script\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"script.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"eval_batch_size\":8,\"model_id\":\"bert-base-uncased\",\"train_batch_size\":8}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=script.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_training_compiler_debug_mode\":false,\"sagemaker_training_compiler_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"testing\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=script\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://agtimeseries/mce-folder/scriptcode1/bert-model-2022-12-06-05-42-55-321/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_training_compiler_debug_mode\":false,\"sagemaker_training_compiler_enabled\":true},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"eval_batch_size\":8,\"model_id\":\"bert-base-uncased\",\"train_batch_size\":8},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"bert-model-2022-12-06-05-42-55-321\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://agtimeseries/mce-folder/scriptcode1/bert-model-2022-12-06-05-42-55-321/source/sourcedir.tar.gz\",\"module_name\":\"script\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"script.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--eval_batch_size\",\"8\",\"--model_id\",\"bert-base-uncased\",\"--train_batch_size\",\"8\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=bert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python script.py --epochs 1 --eval_batch_size 8 --model_id bert-base-uncased --train_batch_size 8\u001b[0m\n",
      "\u001b[34m[2022-12-06 05:49:51.127 torch.__training_compiler__.TrainingCompilerConfig INFO] Found configuration for Training Compiler. Compiler will be configured during import of torch_xla.\u001b[0m\n",
      "\u001b[34m[2022-12-06 05:49:52.108 torch_xla.__training_compiler__.TrainingCompilerConfig INFO] Configuring SM Training Compiler...\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 34.9kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/570 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 570/570 [00:00<00:00, 683kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/226k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 226k/226k [00:00<00:00, 4.66MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/455k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 455k/455k [00:00<00:00, 7.57MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/420M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 10.1M/420M [00:00<00:04, 106MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▌         | 21.2M/420M [00:00<00:03, 112MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   8%|▊         | 32.3M/420M [00:00<00:03, 114MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  10%|█         | 43.3M/420M [00:00<00:03, 115MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 54.4M/420M [00:00<00:03, 115MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▌        | 65.4M/420M [00:00<00:03, 114MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 76.3M/420M [00:00<00:03, 113MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██        | 87.4M/420M [00:00<00:03, 114MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 98.4M/420M [00:00<00:02, 115MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  26%|██▌       | 110M/420M [00:01<00:02, 115MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▊       | 121M/420M [00:01<00:02, 115MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███▏      | 132M/420M [00:01<00:02, 116MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  34%|███▍      | 143M/420M [00:01<00:02, 116MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 154M/420M [00:01<00:02, 116MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  39%|███▉      | 165M/420M [00:01<00:02, 116MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 176M/420M [00:01<00:02, 115MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▍     | 187M/420M [00:01<00:02, 115MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 198M/420M [00:01<00:02, 115MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  50%|████▉     | 209M/420M [00:01<00:01, 116MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 220M/420M [00:02<00:01, 116MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▌    | 231M/420M [00:02<00:01, 116MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  58%|█████▊    | 242M/420M [00:02<00:01, 115MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|██████    | 254M/420M [00:02<00:01, 116MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 265M/420M [00:02<00:01, 116MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  66%|██████▌   | 276M/420M [00:02<00:01, 116MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 287M/420M [00:02<00:01, 115MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  71%|███████   | 298M/420M [00:02<00:01, 115MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▎  | 309M/420M [00:02<00:01, 115MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▌  | 320M/420M [00:02<00:00, 115MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▉  | 331M/420M [00:03<00:00, 116MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████▏ | 342M/420M [00:03<00:00, 116MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▍ | 353M/420M [00:03<00:00, 116MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 364M/420M [00:03<00:00, 116MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▉ | 375M/420M [00:03<00:00, 116MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 386M/420M [00:03<00:00, 113MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▍| 397M/420M [00:03<00:00, 113MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 408M/420M [00:03<00:00, 114MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|█████████▉| 419M/420M [00:03<00:00, 115MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 420M/420M [00:03<00:00, 115MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m2022-12-06 05:50:02.594218: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 93763584 exceeds 10% of free system memory.\u001b[0m\n",
      "\u001b[34mUsing XLA device\u001b[0m\n",
      "\u001b[34mUsing XLA device\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 856\u001b[0m\n",
      "\u001b[34mNum Epochs = 1\n",
      "  Instantaneous batch size per device = 8\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 856\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 107\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 107\u001b[0m\n",
      "\u001b[34m0%|          | 0/107 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2022-12-06 05:50:03.329 algo-1:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-12-06 05:50:03.480 algo-1:27 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m1%|          | 1/107 [00:00<00:57,  1.86it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2%|▏         | 2/107 [00:58<1:00:12, 34.41s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 3/107 [01:57<1:18:48, 45.47s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 4/107 [01:57<47:40, 27.77s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 5/107 [01:58<30:35, 18.00s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 6/107 [01:59<20:22, 12.10s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 7/107 [01:59<13:56,  8.36s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 8/107 [02:00<09:45,  5.91s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 9/107 [02:01<06:57,  4.26s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 10/107 [02:01<05:05,  3.15s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 11/107 [02:02<03:48,  2.38s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 12/107 [02:03<02:56,  1.86s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 13/107 [02:03<02:20,  1.49s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 14/107 [02:04<01:55,  1.24s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 15/107 [02:05<01:37,  1.06s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 16/107 [02:05<01:25,  1.06it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 17/107 [02:06<01:17,  1.17it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 18/107 [02:07<01:10,  1.26it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 19/107 [02:07<01:07,  1.30it/s]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 20/107 [02:08<01:03,  1.36it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 21/107 [02:09<01:00,  1.42it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 22/107 [02:09<00:58,  1.45it/s]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 23/107 [02:10<00:56,  1.48it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 24/107 [02:11<00:55,  1.50it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 25/107 [02:11<00:54,  1.52it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 26/107 [02:12<00:53,  1.52it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 27/107 [02:12<00:52,  1.53it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 28/107 [02:13<00:51,  1.54it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 29/107 [02:14<00:50,  1.54it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 30/107 [02:14<00:50,  1.53it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 31/107 [02:15<00:49,  1.53it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 32/107 [02:16<00:48,  1.54it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 33/107 [02:16<00:47,  1.54it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 34/107 [02:17<00:47,  1.54it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 35/107 [02:18<00:46,  1.54it/s]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 36/107 [02:18<00:46,  1.54it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 37/107 [02:19<00:45,  1.53it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 38/107 [02:20<00:44,  1.53it/s]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 39/107 [02:20<00:44,  1.54it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 40/107 [02:21<00:43,  1.54it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 41/107 [02:22<00:42,  1.54it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 42/107 [02:22<00:42,  1.54it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 43/107 [02:23<00:41,  1.55it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 44/107 [02:24<00:40,  1.55it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 45/107 [02:24<00:40,  1.54it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 46/107 [02:25<00:39,  1.54it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 47/107 [02:25<00:38,  1.54it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 48/107 [02:26<00:38,  1.54it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 49/107 [02:27<00:37,  1.54it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 50/107 [02:27<00:37,  1.54it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 51/107 [02:28<00:36,  1.54it/s]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 52/107 [02:29<00:35,  1.54it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 53/107 [02:29<00:35,  1.54it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 54/107 [02:30<00:34,  1.54it/s]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 55/107 [02:31<00:33,  1.53it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 56/107 [02:31<00:33,  1.53it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 57/107 [02:32<00:32,  1.53it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 58/107 [02:33<00:32,  1.53it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 59/107 [02:33<00:31,  1.53it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 60/107 [02:34<00:30,  1.53it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 61/107 [02:35<00:29,  1.53it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 62/107 [02:35<00:29,  1.53it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 63/107 [02:36<00:28,  1.53it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 64/107 [02:37<00:28,  1.53it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 65/107 [02:37<00:27,  1.53it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 66/107 [02:38<00:26,  1.53it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 67/107 [02:39<00:26,  1.52it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 68/107 [02:39<00:25,  1.52it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 69/107 [02:40<00:24,  1.53it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 70/107 [02:40<00:24,  1.53it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 71/107 [02:41<00:23,  1.53it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 72/107 [02:42<00:22,  1.53it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 73/107 [02:42<00:22,  1.52it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 74/107 [02:43<00:21,  1.53it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 75/107 [02:44<00:20,  1.53it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 76/107 [02:44<00:20,  1.53it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 77/107 [02:45<00:19,  1.53it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 78/107 [02:46<00:18,  1.53it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 79/107 [02:46<00:18,  1.53it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 80/107 [02:47<00:17,  1.53it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 81/107 [02:48<00:16,  1.53it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 82/107 [02:48<00:16,  1.52it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 83/107 [02:49<00:15,  1.52it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 84/107 [02:50<00:15,  1.52it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 85/107 [02:50<00:14,  1.52it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 86/107 [02:51<00:13,  1.53it/s]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 87/107 [02:52<00:13,  1.51it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 88/107 [02:52<00:12,  1.51it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 89/107 [02:53<00:11,  1.52it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 90/107 [02:54<00:11,  1.52it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 91/107 [02:54<00:10,  1.52it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 92/107 [02:55<00:09,  1.52it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 93/107 [02:56<00:09,  1.53it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 94/107 [02:56<00:08,  1.53it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 95/107 [02:57<00:07,  1.54it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 96/107 [02:58<00:07,  1.54it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 97/107 [02:58<00:06,  1.53it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 98/107 [02:59<00:05,  1.52it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 99/107 [03:00<00:05,  1.52it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 100/107 [03:00<00:04,  1.52it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 101/107 [03:01<00:03,  1.52it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 102/107 [03:01<00:03,  1.53it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 103/107 [04:02<01:14, 18.67s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 104/107 [04:03<00:39, 13.27s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 105/107 [04:04<00:18,  9.48s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 106/107 [04:04<00:06,  6.84s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 107/107 [04:05<00:00,  4.98s/it]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 213\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 213\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/27 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 2/27 [00:00<00:02,  8.53it/s]#033[A\u001b[0m\n",
      "\u001b[34m11%|█         | 3/27 [00:00<00:04,  5.98it/s]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▍        | 4/27 [00:00<00:04,  5.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m19%|█▊        | 5/27 [00:00<00:04,  4.76it/s]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 6/27 [00:01<00:04,  4.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▌       | 7/27 [00:01<00:04,  4.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m30%|██▉       | 8/27 [00:01<00:04,  4.33it/s]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 9/27 [00:01<00:04,  4.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 10/27 [00:02<00:03,  4.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m41%|████      | 11/27 [00:02<00:03,  4.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 12/27 [00:02<00:03,  4.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 13/27 [00:02<00:03,  4.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 14/27 [00:03<00:03,  4.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 15/27 [00:03<00:02,  4.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 16/27 [00:03<00:02,  4.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 17/27 [00:03<00:02,  4.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 18/27 [00:04<00:02,  4.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m70%|███████   | 19/27 [00:04<00:01,  4.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 20/27 [00:04<00:01,  4.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 21/27 [00:04<00:01,  4.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 22/27 [00:05<00:01,  4.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 23/27 [00:05<00:00,  4.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 24/27 [00:05<00:00,  4.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 25/27 [00:05<00:00,  4.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 26/27 [00:05<00:00,  4.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 27/27 [00:09<00:00,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 2.48561429977417, 'eval_accuracy': 0.2863849765258216, 'eval_runtime': 13.0248, 'eval_samples_per_second': 16.353, 'eval_steps_per_second': 2.073, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 107/107 [04:18<00:00,  4.98s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 27/27 [00:09<00:00,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m#015                                               #033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to ./results/checkpoint-107\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to ./results/checkpoint-107\u001b[0m\n",
      "\u001b[34mConfiguration saved in ./results/checkpoint-107/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in ./results/checkpoint-107/config.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mModel weights saved in ./results/checkpoint-107/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in ./results/checkpoint-107/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in ./results/checkpoint-107/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in ./results/checkpoint-107/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in ./results/checkpoint-107/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in ./results/checkpoint-107/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mLoading best model from ./results/checkpoint-107 (score: 2.48561429977417).\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mLoading best model from ./results/checkpoint-107 (score: 2.48561429977417).\u001b[0m\n",
      "\u001b[34m{'train_runtime': 263.8688, 'train_samples_per_second': 3.244, 'train_steps_per_second': 0.406, 'train_loss': 2.7922003915376754, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 107/107 [04:23<00:00,  4.98s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 107/107 [04:23<00:00,  2.47s/it]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 213\u001b[0m\n",
      "\u001b[34mBatch size = 8\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 213\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 2/27 [00:00<00:02,  8.50it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 3/27 [00:00<00:03,  6.02it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 4/27 [00:00<00:04,  5.09it/s]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 5/27 [00:00<00:04,  4.78it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 6/27 [00:01<00:04,  4.60it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 7/27 [00:01<00:04,  4.48it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 8/27 [00:01<00:04,  4.37it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 9/27 [00:01<00:04,  4.33it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 10/27 [00:02<00:03,  4.31it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 11/27 [00:02<00:03,  4.29it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 12/27 [00:02<00:03,  4.27it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 13/27 [00:02<00:03,  4.27it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 14/27 [00:03<00:03,  4.27it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 15/27 [00:03<00:02,  4.27it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 16/27 [00:03<00:02,  4.26it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 17/27 [00:03<00:02,  4.26it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 18/27 [00:04<00:02,  4.27it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 19/27 [00:04<00:01,  4.27it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 20/27 [00:04<00:01,  4.27it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 21/27 [00:04<00:01,  4.26it/s]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 22/27 [00:04<00:01,  4.27it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 23/27 [00:05<00:00,  4.27it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 24/27 [00:05<00:00,  4.28it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 25/27 [00:05<00:00,  4.27it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 26/27 [00:05<00:00,  4.28it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 27/27 [00:06<00:00,  4.81it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 27/27 [00:06<00:00,  4.48it/s]\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m2022-12-06 05:54:37,095 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-12-06 05:54:37,095 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-12-06 05:54:37,095 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-12-06 05:55:40 Uploading - Uploading generated training model\n",
      "2022-12-06 05:56:36 Completed - Training job completed\n",
      "Training seconds: 744\n",
      "Billable seconds: 744\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator1.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7bd87c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://agtimeseries/mce-hf-folder/model1/bert-model-2022-12-06-05-42-55-321/output/model.tar.gz'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator1.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c997978",
   "metadata": {},
   "source": [
    "# Pytorch Estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1879928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch as PyTorchEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "195504c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={'epochs': 1,                                    # number of training epochs\n",
    "                 'train_batch_size': 8,                         # batch size for training\n",
    "                 'eval_batch_size': 8,                          #batch size for testing\n",
    "                 \"model_id\" :'bert-base-uncased'                #pretrained model name from hf\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa395fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_estimator1 = PyTorchEstimator(\n",
    "    entry_point          = 'script.py',       # script code to run the model\n",
    "    source_dir           = 'Pcode/',\n",
    "    instance_type        = 'ml.g4dn.xlarge',    # instances type used for the training job\n",
    "    instance_count       = 1,                  # the number of instances used for training\n",
    "    base_job_name        = \"bert-model\",       # the name of the training job     \n",
    "    role                 = role,               # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    py_version='py38',                           # the python version used in the training job\n",
    "    framework_version='1.9.0'   ,              # the pytorch_version version used in the training job       \n",
    "    hyperparameters      = hyperparameters,    # the hyperparameter used for running the training job\n",
    "    disable_profiler     = True,               # whether to disable the profiler during training used to gain maximum performance\n",
    "    debugger_hook_config = False,              # whether to enable the debugger hook during training used to gain maximum performance\n",
    "    output_path          = model_data_loc2,     # s3 location where the final fine-tuned model will be saved \n",
    "    code_location        = script_code_loc2     # s3 location where the estimator will push the script code\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26038e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'training': training_data_loc, 'testing': testing_data_loc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8f5cd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-06 06:28:18 Starting - Starting the training job...\n",
      "2022-12-06 06:28:33 Starting - Preparing the instances for training......\n",
      "2022-12-06 06:29:48 Downloading - Downloading input data\n",
      "2022-12-06 06:29:48 Training - Downloading the training image..............................\n",
      "2022-12-06 06:34:39 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-12-06 06:34:52,002 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-12-06 06:34:52,028 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-12-06 06:34:52,031 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-12-06 06:34:52,255 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"eval_batch_size\": 8,\n",
      "        \"model_id\": \"bert-base-uncased\",\n",
      "        \"train_batch_size\": 8\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"bert-model-2022-12-06-06-28-18-002\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://agtimeseries/mce-folder/scriptcode2/bert-model-2022-12-06-06-28-18-002/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"script\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"script.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"eval_batch_size\":8,\"model_id\":\"bert-base-uncased\",\"train_batch_size\":8}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=script.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"testing\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=script\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://agtimeseries/mce-folder/scriptcode2/bert-model-2022-12-06-06-28-18-002/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"eval_batch_size\":8,\"model_id\":\"bert-base-uncased\",\"train_batch_size\":8},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"bert-model-2022-12-06-06-28-18-002\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://agtimeseries/mce-folder/scriptcode2/bert-model-2022-12-06-06-28-18-002/source/sourcedir.tar.gz\",\"module_name\":\"script\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"script.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--eval_batch_size\",\"8\",\"--model_id\",\"bert-base-uncased\",\"--train_batch_size\",\"8\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=bert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 script.py --epochs 1 --eval_batch_size 8 --model_id bert-base-uncased --train_batch_size 8\u001b[0m\n",
      "\u001b[34mCollecting datasets\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.7.1-py3-none-any.whl (451 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.12.2)\u001b[0m\n",
      "\u001b[34mCollecting xxhash\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.0)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow>=6.0.0\u001b[0m\n",
      "\u001b[34mDownloading pyarrow-10.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.0 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2.26.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets) (1.19.1)\u001b[0m\n",
      "\u001b[34mCollecting fsspec[http]>=2021.11.1\u001b[0m\n",
      "\u001b[34mDownloading fsspec-2022.11.0-py3-none-any.whl (139 kB)\u001b[0m\n",
      "\u001b[34mCollecting tqdm>=4.62.1\u001b[0m\n",
      "\u001b[34mDownloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.2.4)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0.0,>=0.2.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (2.0.4)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting filelock\u001b[0m\n",
      "\u001b[34mDownloading filelock-3.8.2-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: multidict, frozenlist, yarl, async-timeout, aiosignal, tqdm, fsspec, filelock, aiohttp, xxhash, responses, pyarrow, huggingface-hub, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tqdm\u001b[0m\n",
      "\u001b[34mFound existing installation: tqdm 4.61.2\u001b[0m\n",
      "\u001b[34mUninstalling tqdm-4.61.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tqdm-4.61.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: fsspec\u001b[0m\n",
      "\u001b[34mFound existing installation: fsspec 2021.10.0\u001b[0m\n",
      "\u001b[34mUninstalling fsspec-2021.10.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled fsspec-2021.10.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: pyarrow\u001b[0m\n",
      "\u001b[34mFound existing installation: pyarrow 5.0.0\u001b[0m\n",
      "\u001b[34mUninstalling pyarrow-5.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled pyarrow-5.0.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiohttp-3.8.3 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.7.1 filelock-3.8.2 frozenlist-1.3.3 fsspec-2022.11.0 huggingface-hub-0.11.1 multidict-6.0.3 pyarrow-10.0.1 responses-0.18.0 tqdm-4.64.1 xxhash-3.1.0 yarl-1.8.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mCollecting transformers\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.19.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\u001b[0m\n",
      "\u001b[34mDownloading regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.4)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, regex, transformers\u001b[0m\n",
      "\u001b[34mSuccessfully installed regex-2022.10.31 tokenizers-0.13.2 transformers-4.25.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mCollecting simpletransformers\u001b[0m\n",
      "\u001b[34mDownloading simpletransformers-0.63.9-py3-none-any.whl (250 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex in /opt/conda/lib/python3.8/site-packages (from simpletransformers) (2022.10.31)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers>=4.6.0 in /opt/conda/lib/python3.8/site-packages (from simpletransformers) (4.25.1)\u001b[0m\n",
      "\u001b[34mCollecting seqeval\u001b[0m\n",
      "\u001b[34mDownloading seqeval-1.2.2.tar.gz (43 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (from simpletransformers) (2.7.1)\u001b[0m\n",
      "\u001b[34mCollecting streamlit\u001b[0m\n",
      "\u001b[34mDownloading streamlit-1.15.2-py2.py3-none-any.whl (9.2 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from simpletransformers) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers in /opt/conda/lib/python3.8/site-packages (from simpletransformers) (0.13.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.47.0 in /opt/conda/lib/python3.8/site-packages (from simpletransformers) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (from simpletransformers) (0.24.2)\u001b[0m\n",
      "\u001b[34mCollecting wandb>=0.10.32\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.13.5-py2.py3-none-any.whl (1.9 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from simpletransformers) (1.2.4)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from simpletransformers) (2.26.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from simpletransformers) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.8/site-packages (from transformers>=4.6.0->simpletransformers) (0.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers>=4.6.0->simpletransformers) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers>=4.6.0->simpletransformers) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers>=4.6.0->simpletransformers) (3.8.2)\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting pathtools\u001b[0m\n",
      "\u001b[34mDownloading pathtools-0.1.2.tar.gz (11 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-1.11.1-py2.py3-none-any.whl (168 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /opt/conda/lib/python3.8/site-packages (from wandb>=0.10.32->simpletransformers) (3.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.8/site-packages (from wandb>=0.10.32->simpletransformers) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb>=0.10.32->simpletransformers) (5.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from wandb>=0.10.32->simpletransformers) (58.2.0)\u001b[0m\n",
      "\u001b[34mCollecting shortuuid>=0.5.0\u001b[0m\n",
      "\u001b[34mDownloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting promise<3,>=2.0\u001b[0m\n",
      "\u001b[34mDownloading promise-2.3.tar.gz (19 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting setproctitle\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.8/site-packages (from wandb>=0.10.32->simpletransformers) (8.0.3)\u001b[0m\n",
      "\u001b[34mCollecting GitPython>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.29-py3-none-any.whl (182 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->simpletransformers) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->simpletransformers) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->simpletransformers) (2021.10.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->simpletransformers) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets->simpletransformers) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets->simpletransformers) (3.8.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from datasets->simpletransformers) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets->simpletransformers) (10.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.8/site-packages (from datasets->simpletransformers) (0.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.8/site-packages (from datasets->simpletransformers) (2022.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets->simpletransformers) (0.70.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->simpletransformers) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->simpletransformers) (2021.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->simpletransformers) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->simpletransformers) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.10.0.0 in /opt/conda/lib/python3.8/site-packages (from streamlit->simpletransformers) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mCollecting blinker>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading blinker-1.5-py2.py3-none-any.whl (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting pympler>=0.9\u001b[0m\n",
      "\u001b[34mDownloading Pympler-1.0.1-py3-none-any.whl (164 kB)\u001b[0m\n",
      "\u001b[34mCollecting altair>=3.2.0\u001b[0m\n",
      "\u001b[34mDownloading altair-4.2.0-py3-none-any.whl (812 kB)\u001b[0m\n",
      "\u001b[34mCollecting tzlocal>=1.1\u001b[0m\n",
      "\u001b[34mDownloading tzlocal-4.2-py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from streamlit->simpletransformers) (8.3.2)\u001b[0m\n",
      "\u001b[34mCollecting rich>=10.11.0\u001b[0m\n",
      "\u001b[34mDownloading rich-12.6.0-py3-none-any.whl (237 kB)\u001b[0m\n",
      "\u001b[34mCollecting watchdog\u001b[0m\n",
      "\u001b[34mDownloading watchdog-2.2.0-py3-none-manylinux2014_x86_64.whl (78 kB)\u001b[0m\n",
      "\u001b[34mCollecting pydeck>=0.1.dev5\u001b[0m\n",
      "\u001b[34mDownloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\u001b[0m\n",
      "\u001b[34mCollecting validators>=0.2\u001b[0m\n",
      "\u001b[34mDownloading validators-0.20.0.tar.gz (30 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting semver\u001b[0m\n",
      "\u001b[34mDownloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=1.4 in /opt/conda/lib/python3.8/site-packages (from streamlit->simpletransformers) (4.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tornado>=5.0 in /opt/conda/lib/python3.8/site-packages (from streamlit->simpletransformers) (6.1)\u001b[0m\n",
      "\u001b[34mCollecting cachetools>=4.0\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting toml\u001b[0m\n",
      "\u001b[34mDownloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\u001b[0m\n",
      "\u001b[34mDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.4.1-py3-none-any.whl (93 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard->simpletransformers) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard->simpletransformers) (0.36.2)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.7.0,>=0.6.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.3.0-py3-none-any.whl (124 kB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.15.0-py2.py3-none-any.whl (177 kB)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.24.3\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.51.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from altair>=3.2.0->streamlit->simpletransformers) (3.0.2)\u001b[0m\n",
      "\u001b[34mCollecting toolz\u001b[0m\n",
      "\u001b[34mDownloading toolz-0.12.0-py3-none-any.whl (55 kB)\u001b[0m\n",
      "\u001b[34mCollecting entrypoints\u001b[0m\n",
      "\u001b[34mDownloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.8/site-packages (from altair>=3.2.0->streamlit->simpletransformers) (4.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->simpletransformers) (21.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->simpletransformers) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->simpletransformers) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->simpletransformers) (1.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->simpletransformers) (6.0.3)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.10-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\u001b[0m\n",
      "\u001b[34mDownloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.7.2)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=1.4->streamlit->simpletransformers) (3.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers>=4.6.0->simpletransformers) (2.4.7)\u001b[0m\n",
      "\u001b[34mCollecting commonmark<0.10.0,>=0.9.0\u001b[0m\n",
      "\u001b[34mDownloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/lib/python3.8/site-packages (from rich>=10.11.0->streamlit->simpletransformers) (2.7.1)\u001b[0m\n",
      "\u001b[34mCollecting typing-extensions>=3.10.0.0\u001b[0m\n",
      "\u001b[34mDownloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting urllib3<1.27,>=1.21.1\u001b[0m\n",
      "\u001b[34mDownloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\u001b[0m\n",
      "\u001b[34mCollecting pytz-deprecation-shim\u001b[0m\n",
      "\u001b[34mDownloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mCollecting backports.zoneinfo\u001b[0m\n",
      "\u001b[34mDownloading backports.zoneinfo-0.2.1-cp38-cp38-manylinux1_x86_64.whl (74 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: decorator>=3.4.0 in /opt/conda/lib/python3.8/site-packages (from validators>=0.2->streamlit->simpletransformers) (4.4.2)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->altair>=3.2.0->streamlit->simpletransformers) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.4.8)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\u001b[0m\n",
      "\u001b[34mDownloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34mCollecting tzdata\u001b[0m\n",
      "\u001b[34mDownloading tzdata-2022.7-py2.py3-none-any.whl (340 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: seqeval, promise, validators, pathtools\u001b[0m\n",
      "\u001b[34mBuilding wheel for seqeval (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for seqeval (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=3732e91f24fec1f2b7efc44c676789a1bbd4c984931923103625679ab401f503\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\u001b[0m\n",
      "\u001b[34mBuilding wheel for promise (setup.py): started\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mBuilding wheel for promise (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=4dc6131383ceb00ed56f432dbd3480fd690943a75a89ac169abcace3222a1f85\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\u001b[0m\n",
      "\u001b[34mBuilding wheel for validators (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for validators (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19581 sha256=401a915acf6964bcafec46b5d5988f15c6b280c373447e9ea7e1351c40aaa404\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/19/09/72/3eb74d236bb48bd0f3c6c3c83e4e0c5bbfcbcad7c6c3539db8\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=837b7bb404dab5865307101f5dcdb7311145916dc2766e9ef1e2a7fe965552cc\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\u001b[0m\n",
      "\u001b[34mSuccessfully built seqeval promise validators pathtools\u001b[0m\n",
      "\u001b[34mInstalling collected packages: urllib3, tzdata, smmap, pyasn1-modules, oauthlib, cachetools, backports.zoneinfo, typing-extensions, toolz, requests-oauthlib, pytz-deprecation-shim, google-auth, gitdb, entrypoints, commonmark, watchdog, validators, tzlocal, toml, tensorboard-plugin-wit, tensorboard-data-server, shortuuid, setproctitle, sentry-sdk, semver, rich, pympler, pydeck, promise, pathtools, markdown, grpcio, google-auth-oauthlib, GitPython, docker-pycreds, blinker, altair, absl-py, wandb, tensorboard, streamlit, seqeval, sentencepiece, simpletransformers\u001b[0m\n",
      "\u001b[34mAttempting uninstall: urllib3\u001b[0m\n",
      "\u001b[34mFound existing installation: urllib3 1.26.6\u001b[0m\n",
      "\u001b[34mUninstalling urllib3-1.26.6:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled urllib3-1.26.6\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[34mFound existing installation: typing-extensions 3.10.0.2\u001b[0m\n",
      "\u001b[34mUninstalling typing-extensions-3.10.0.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typing-extensions-3.10.0.2\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mjsii 1.39.0 requires typing-extensions~=3.7, but you have typing-extensions 4.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed GitPython-3.1.29 absl-py-1.3.0 altair-4.2.0 backports.zoneinfo-0.2.1 blinker-1.5 cachetools-5.2.0 commonmark-0.9.1 docker-pycreds-0.4.0 entrypoints-0.4 gitdb-4.0.10 google-auth-2.15.0 google-auth-oauthlib-0.4.6 grpcio-1.51.1 markdown-3.4.1 oauthlib-3.2.2 pathtools-0.1.2 promise-2.3 pyasn1-modules-0.2.8 pydeck-0.8.0 pympler-1.0.1 pytz-deprecation-shim-0.1.0.post0 requests-oauthlib-1.3.1 rich-12.6.0 semver-2.13.0 sentencepiece-0.1.97 sentry-sdk-1.11.1 seqeval-1.2.2 setproctitle-1.3.2 shortuuid-1.0.11 simpletransformers-0.63.9 smmap-5.0.0 streamlit-1.15.2 tensorboard-2.11.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 toml-0.10.2 toolz-0.12.0 typing-extensions-4.4.0 tzdata-2022.7 tzlocal-4.2 urllib3-1.26.13 validators-0.20.0 wandb-0.13.5 watchdog-2.2.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/570 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 570/570 [00:00<00:00, 780kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/440M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 7.30M/440M [00:00<00:05, 73.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▍         | 17.7M/440M [00:00<00:04, 91.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▋         | 28.1M/440M [00:00<00:04, 96.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 38.6M/440M [00:00<00:04, 100MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█         | 49.2M/440M [00:00<00:03, 102MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 59.5M/440M [00:00<00:03, 102MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▌        | 69.7M/440M [00:00<00:03, 102MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 80.2M/440M [00:00<00:03, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██        | 90.7M/440M [00:00<00:03, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 101M/440M [00:01<00:03, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▌       | 112M/440M [00:01<00:03, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  28%|██▊       | 122M/440M [00:01<00:03, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  30%|███       | 132M/440M [00:01<00:02, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  32%|███▏      | 143M/440M [00:01<00:02, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▍      | 153M/440M [00:01<00:02, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 164M/440M [00:01<00:02, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  40%|███▉      | 174M/440M [00:01<00:02, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 185M/440M [00:01<00:02, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  44%|████▍     | 195M/440M [00:01<00:02, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 206M/440M [00:02<00:02, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▉     | 216M/440M [00:02<00:02, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████▏    | 227M/440M [00:02<00:02, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▍    | 237M/440M [00:02<00:01, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▌    | 247M/440M [00:02<00:01, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▊    | 258M/440M [00:02<00:01, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████    | 268M/440M [00:02<00:01, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 279M/440M [00:02<00:01, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  66%|██████▌   | 289M/440M [00:02<00:01, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 300M/440M [00:02<00:01, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|███████   | 310M/440M [00:03<00:01, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  73%|███████▎  | 321M/440M [00:03<00:01, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  75%|███████▌  | 331M/440M [00:03<00:01, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  78%|███████▊  | 342M/440M [00:03<00:00, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  80%|███████▉  | 352M/440M [00:03<00:00, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 363M/440M [00:03<00:00, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▍ | 373M/440M [00:03<00:00, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 384M/440M [00:03<00:00, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  90%|████████▉ | 395M/440M [00:03<00:00, 106MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 406M/440M [00:03<00:00, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▍| 417M/440M [00:04<00:00, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 427M/440M [00:04<00:00, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|█████████▉| 438M/440M [00:04<00:00, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 440M/440M [00:04<00:00, 104MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 36.0kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/232k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 232k/232k [00:00<00:00, 5.22MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/466k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 466k/466k [00:00<00:00, 8.31MB/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/simpletransformers/classification/classification_model.py:612: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m0%|          | 0/856 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1/856 [00:00<04:59,  2.86it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 2/856 [00:00<02:29,  5.71it/s]\u001b[0m\n",
      "\u001b[34mEpoch:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mEpoch 1 of 1:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mRunning Epoch 0 of 1:   0%|          | 0/107 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m[2022-12-06 06:35:34.518 algo-1:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-12-06 06:35:34.538 algo-1:27 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.0305:   0%|          | 0/107 [00:01<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/simpletransformers/classification/classification_model.py:949: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.0305:   1%|          | 1/107 [00:01<02:34,  1.46s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9246:   1%|          | 1/107 [00:01<02:34,  1.46s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9246:   2%|▏         | 2/107 [00:01<01:10,  1.48it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.1931:   2%|▏         | 2/107 [00:01<01:10,  1.48it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.1931:   3%|▎         | 3/107 [00:01<00:42,  2.47it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.0762:   3%|▎         | 3/107 [00:01<00:42,  2.47it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.0762:   4%|▎         | 4/107 [00:01<00:29,  3.45it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.3557:   4%|▎         | 4/107 [00:01<00:29,  3.45it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.3557:   5%|▍         | 5/107 [00:01<00:23,  4.40it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.0242:   5%|▍         | 5/107 [00:01<00:23,  4.40it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.0242:   6%|▌         | 6/107 [00:02<00:19,  5.25it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9756:   6%|▌         | 6/107 [00:02<00:19,  5.25it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9756:   7%|▋         | 7/107 [00:02<00:16,  6.01it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.1343:   7%|▋         | 7/107 [00:02<00:16,  6.01it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.1343:   7%|▋         | 8/107 [00:02<00:14,  6.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.8667:   7%|▋         | 8/107 [00:02<00:14,  6.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.8667:   8%|▊         | 9/107 [00:02<00:13,  7.17it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9209:   8%|▊         | 9/107 [00:02<00:13,  7.17it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9209:   9%|▉         | 10/107 [00:02<00:12,  7.56it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9993:   9%|▉         | 10/107 [00:02<00:12,  7.56it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9993:  10%|█         | 11/107 [00:02<00:12,  7.87it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.1035:  10%|█         | 11/107 [00:02<00:12,  7.87it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.1035:  11%|█         | 12/107 [00:02<00:11,  8.08it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.0391:  11%|█         | 12/107 [00:02<00:11,  8.08it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.0391:  12%|█▏        | 13/107 [00:02<00:11,  8.23it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9839:  12%|█▏        | 13/107 [00:02<00:11,  8.23it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9839:  13%|█▎        | 14/107 [00:02<00:11,  8.34it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9595:  13%|█▎        | 14/107 [00:02<00:11,  8.34it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9595:  14%|█▍        | 15/107 [00:03<00:10,  8.41it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.0286:  14%|█▍        | 15/107 [00:03<00:10,  8.41it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.0286:  15%|█▍        | 16/107 [00:03<00:10,  8.45it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.1108:  15%|█▍        | 16/107 [00:03<00:10,  8.45it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.1108:  16%|█▌        | 17/107 [00:03<00:10,  8.49it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9978:  16%|█▌        | 17/107 [00:03<00:10,  8.49it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9978:  17%|█▋        | 18/107 [00:03<00:10,  8.54it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9150:  17%|█▋        | 18/107 [00:03<00:10,  8.54it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9150:  18%|█▊        | 19/107 [00:03<00:10,  8.55it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.0618:  18%|█▊        | 19/107 [00:03<00:10,  8.55it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.0618:  19%|█▊        | 20/107 [00:03<00:10,  8.56it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9817:  19%|█▊        | 20/107 [00:03<00:10,  8.56it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9817:  20%|█▉        | 21/107 [00:03<00:10,  8.57it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.8257:  20%|█▉        | 21/107 [00:03<00:10,  8.57it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.8257:  21%|██        | 22/107 [00:03<00:09,  8.60it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.8792:  21%|██        | 22/107 [00:03<00:09,  8.60it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.8792:  21%|██▏       | 23/107 [00:03<00:09,  8.60it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7424:  21%|██▏       | 23/107 [00:04<00:09,  8.60it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7424:  22%|██▏       | 24/107 [00:04<00:09,  8.60it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9233:  22%|██▏       | 24/107 [00:04<00:09,  8.60it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9233:  23%|██▎       | 25/107 [00:04<00:09,  8.63it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9292:  23%|██▎       | 25/107 [00:04<00:09,  8.63it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9292:  24%|██▍       | 26/107 [00:04<00:09,  8.63it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.8589:  24%|██▍       | 26/107 [00:04<00:09,  8.63it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.8589:  25%|██▌       | 27/107 [00:04<00:09,  8.60it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.8677:  25%|██▌       | 27/107 [00:04<00:09,  8.60it/s]#033[A\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpochs 0/1. Running Loss:    2.8677:  26%|██▌       | 28/107 [00:04<00:09,  8.61it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9211:  26%|██▌       | 28/107 [00:04<00:09,  8.61it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9211:  27%|██▋       | 29/107 [00:04<00:09,  8.59it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.8550:  27%|██▋       | 29/107 [00:04<00:09,  8.59it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.8550:  28%|██▊       | 30/107 [00:04<00:08,  8.59it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6743:  28%|██▊       | 30/107 [00:04<00:08,  8.59it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6743:  29%|██▉       | 31/107 [00:04<00:08,  8.60it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9917:  29%|██▉       | 31/107 [00:04<00:08,  8.60it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9917:  30%|██▉       | 32/107 [00:05<00:08,  8.62it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7878:  30%|██▉       | 32/107 [00:05<00:08,  8.62it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7878:  31%|███       | 33/107 [00:05<00:08,  8.63it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.1729:  31%|███       | 33/107 [00:05<00:08,  8.63it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.1729:  32%|███▏      | 34/107 [00:05<00:08,  8.63it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7712:  32%|███▏      | 34/107 [00:05<00:08,  8.63it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7712:  33%|███▎      | 35/107 [00:05<00:08,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.0952:  33%|███▎      | 35/107 [00:05<00:08,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.0952:  34%|███▎      | 36/107 [00:05<00:08,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9424:  34%|███▎      | 36/107 [00:05<00:08,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9424:  35%|███▍      | 37/107 [00:05<00:08,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9822:  35%|███▍      | 37/107 [00:05<00:08,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9822:  36%|███▌      | 38/107 [00:05<00:07,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9624:  36%|███▌      | 38/107 [00:05<00:07,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9624:  36%|███▋      | 39/107 [00:05<00:07,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7158:  36%|███▋      | 39/107 [00:05<00:07,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7158:  37%|███▋      | 40/107 [00:05<00:07,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.8362:  37%|███▋      | 40/107 [00:05<00:07,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.8362:  38%|███▊      | 41/107 [00:06<00:07,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7778:  38%|███▊      | 41/107 [00:06<00:07,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7778:  39%|███▉      | 42/107 [00:06<00:07,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9937:  39%|███▉      | 42/107 [00:06<00:07,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9937:  40%|████      | 43/107 [00:06<00:07,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7629:  40%|████      | 43/107 [00:06<00:07,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7629:  41%|████      | 44/107 [00:06<00:07,  8.68it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9758:  41%|████      | 44/107 [00:06<00:07,  8.68it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9758:  42%|████▏     | 45/107 [00:06<00:07,  8.68it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9338:  42%|████▏     | 45/107 [00:06<00:07,  8.68it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.9338:  43%|████▎     | 46/107 [00:06<00:07,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6318:  43%|████▎     | 46/107 [00:06<00:07,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6318:  44%|████▍     | 47/107 [00:06<00:06,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7832:  44%|████▍     | 47/107 [00:06<00:06,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7832:  45%|████▍     | 48/107 [00:06<00:06,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7576:  45%|████▍     | 48/107 [00:06<00:06,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7576:  46%|████▌     | 49/107 [00:06<00:06,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6511:  46%|████▌     | 49/107 [00:07<00:06,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6511:  47%|████▋     | 50/107 [00:07<00:06,  8.60it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7556:  47%|████▋     | 50/107 [00:07<00:06,  8.60it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7556:  48%|████▊     | 51/107 [00:07<00:06,  8.61it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6555:  48%|████▊     | 51/107 [00:07<00:06,  8.61it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6555:  49%|████▊     | 52/107 [00:07<00:06,  8.62it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.8225:  49%|████▊     | 52/107 [00:07<00:06,  8.62it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.8225:  50%|████▉     | 53/107 [00:07<00:06,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7073:  50%|████▉     | 53/107 [00:07<00:06,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7073:  50%|█████     | 54/107 [00:07<00:06,  8.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.8345:  50%|█████     | 54/107 [00:07<00:06,  8.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.8345:  51%|█████▏    | 55/107 [00:07<00:06,  8.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5444:  51%|█████▏    | 55/107 [00:07<00:06,  8.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5444:  52%|█████▏    | 56/107 [00:07<00:05,  8.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.0095:  52%|█████▏    | 56/107 [00:07<00:05,  8.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    3.0095:  53%|█████▎    | 57/107 [00:07<00:05,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6183:  53%|█████▎    | 57/107 [00:07<00:05,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6183:  54%|█████▍    | 58/107 [00:08<00:05,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7849:  54%|█████▍    | 58/107 [00:08<00:05,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7849:  55%|█████▌    | 59/107 [00:08<00:05,  8.62it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7150:  55%|█████▌    | 59/107 [00:08<00:05,  8.62it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7150:  56%|█████▌    | 60/107 [00:08<00:05,  8.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5403:  56%|█████▌    | 60/107 [00:08<00:05,  8.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5403:  57%|█████▋    | 61/107 [00:08<00:05,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5979:  57%|█████▋    | 61/107 [00:08<00:05,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5979:  58%|█████▊    | 62/107 [00:08<00:05,  8.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6692:  58%|█████▊    | 62/107 [00:08<00:05,  8.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6692:  59%|█████▉    | 63/107 [00:08<00:05,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6544:  59%|█████▉    | 63/107 [00:08<00:05,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6544:  60%|█████▉    | 64/107 [00:08<00:04,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.3353:  60%|█████▉    | 64/107 [00:08<00:04,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.3353:  61%|██████    | 65/107 [00:08<00:04,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4198:  61%|██████    | 65/107 [00:08<00:04,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4198:  62%|██████▏   | 66/107 [00:08<00:04,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6559:  62%|██████▏   | 66/107 [00:08<00:04,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6559:  63%|██████▎   | 67/107 [00:09<00:04,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5696:  63%|██████▎   | 67/107 [00:09<00:04,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5696:  64%|██████▎   | 68/107 [00:09<00:04,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4475:  64%|██████▎   | 68/107 [00:09<00:04,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4475:  64%|██████▍   | 69/107 [00:09<00:04,  8.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5627:  64%|██████▍   | 69/107 [00:09<00:04,  8.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5627:  65%|██████▌   | 70/107 [00:09<00:04,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7292:  65%|██████▌   | 70/107 [00:09<00:04,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7292:  66%|██████▋   | 71/107 [00:09<00:04,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5679:  66%|██████▋   | 71/107 [00:09<00:04,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5679:  67%|██████▋   | 72/107 [00:09<00:04,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4198:  67%|██████▋   | 72/107 [00:09<00:04,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4198:  68%|██████▊   | 73/107 [00:09<00:03,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4935:  68%|██████▊   | 73/107 [00:09<00:03,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4935:  69%|██████▉   | 74/107 [00:09<00:03,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7122:  69%|██████▉   | 74/107 [00:09<00:03,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.7122:  70%|███████   | 75/107 [00:10<00:03,  8.68it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4006:  70%|███████   | 75/107 [00:10<00:03,  8.68it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4006:  71%|███████   | 76/107 [00:10<00:03,  8.68it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6643:  71%|███████   | 76/107 [00:10<00:03,  8.68it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6643:  72%|███████▏  | 77/107 [00:10<00:03,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4216:  72%|███████▏  | 77/107 [00:10<00:03,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4216:  73%|███████▎  | 78/107 [00:10<00:03,  8.70it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.2755:  73%|███████▎  | 78/107 [00:10<00:03,  8.70it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.2755:  74%|███████▍  | 79/107 [00:10<00:03,  8.69it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.2338:  74%|███████▍  | 79/107 [00:10<00:03,  8.69it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.2338:  75%|███████▍  | 80/107 [00:10<00:03,  8.68it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5018:  75%|███████▍  | 80/107 [00:10<00:03,  8.68it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5018:  76%|███████▌  | 81/107 [00:10<00:02,  8.69it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4062:  76%|███████▌  | 81/107 [00:10<00:02,  8.69it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4062:  77%|███████▋  | 82/107 [00:10<00:02,  8.69it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6879:  77%|███████▋  | 82/107 [00:10<00:02,  8.69it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6879:  78%|███████▊  | 83/107 [00:10<00:02,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.3440:  78%|███████▊  | 83/107 [00:10<00:02,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.3440:  79%|███████▊  | 84/107 [00:11<00:02,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.2970:  79%|███████▊  | 84/107 [00:11<00:02,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.2970:  79%|███████▉  | 85/107 [00:11<00:02,  8.63it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.8555:  79%|███████▉  | 85/107 [00:11<00:02,  8.63it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.8555:  80%|████████  | 86/107 [00:11<00:02,  8.48it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6283:  80%|████████  | 86/107 [00:11<00:02,  8.48it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6283:  81%|████████▏ | 87/107 [00:11<00:02,  8.52it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5149:  81%|████████▏ | 87/107 [00:11<00:02,  8.52it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5149:  82%|████████▏ | 88/107 [00:11<00:02,  8.55it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4883:  82%|████████▏ | 88/107 [00:11<00:02,  8.55it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4883:  83%|████████▎ | 89/107 [00:11<00:02,  8.60it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6399:  83%|████████▎ | 89/107 [00:11<00:02,  8.60it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6399:  84%|████████▍ | 90/107 [00:11<00:01,  8.60it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.1215:  84%|████████▍ | 90/107 [00:11<00:01,  8.60it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.1215:  85%|████████▌ | 91/107 [00:11<00:01,  8.61it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.3208:  85%|████████▌ | 91/107 [00:11<00:01,  8.61it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.3208:  86%|████████▌ | 92/107 [00:11<00:01,  8.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.0361:  86%|████████▌ | 92/107 [00:12<00:01,  8.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.0361:  87%|████████▋ | 93/107 [00:12<00:01,  8.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5375:  87%|████████▋ | 93/107 [00:12<00:01,  8.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5375:  88%|████████▊ | 94/107 [00:12<00:01,  8.63it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5420:  88%|████████▊ | 94/107 [00:12<00:01,  8.63it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5420:  89%|████████▉ | 95/107 [00:12<00:01,  8.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5791:  89%|████████▉ | 95/107 [00:12<00:01,  8.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5791:  90%|████████▉ | 96/107 [00:12<00:01,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6741:  90%|████████▉ | 96/107 [00:12<00:01,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6741:  91%|█████████ | 97/107 [00:12<00:01,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4436:  91%|█████████ | 97/107 [00:12<00:01,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4436:  92%|█████████▏| 98/107 [00:12<00:01,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4363:  92%|█████████▏| 98/107 [00:12<00:01,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4363:  93%|█████████▎| 99/107 [00:12<00:00,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4592:  93%|█████████▎| 99/107 [00:12<00:00,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4592:  93%|█████████▎| 100/107 [00:12<00:00,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.1453:  93%|█████████▎| 100/107 [00:12<00:00,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.1453:  94%|█████████▍| 101/107 [00:13<00:00,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.3104:  94%|█████████▍| 101/107 [00:13<00:00,  8.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.3104:  95%|█████████▌| 102/107 [00:13<00:00,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.3416:  95%|█████████▌| 102/107 [00:13<00:00,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.3416:  96%|█████████▋| 103/107 [00:13<00:00,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5322:  96%|█████████▋| 103/107 [00:13<00:00,  8.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.5322:  97%|█████████▋| 104/107 [00:13<00:00,  8.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.3569:  97%|█████████▋| 104/107 [00:13<00:00,  8.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.3569:  98%|█████████▊| 105/107 [00:13<00:00,  8.60it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4893:  98%|█████████▊| 105/107 [00:13<00:00,  8.60it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.4893:  99%|█████████▉| 106/107 [00:13<00:00,  8.62it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6978:  99%|█████████▉| 106/107 [00:13<00:00,  8.62it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6978: 100%|██████████| 107/107 [00:13<00:00,  8.62it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpochs 0/1. Running Loss:    2.6978: 100%|██████████| 107/107 [00:13<00:00,  7.80it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/simpletransformers/classification/classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\u001b[0m\n",
      "\u001b[34m0%|          | 0/213 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m0%|          | 1/213 [00:00<00:25,  8.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m0%|          | 1/213 [00:00<00:25,  8.40it/s]\u001b[0m\n",
      "\u001b[34mINFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_20_2\u001b[0m\n",
      "\u001b[34mEpoch 1 of 1: 100%|██████████| 1/1 [00:22<00:00, 22.26s/it]\u001b[0m\n",
      "\u001b[34mEpoch 1 of 1: 100%|██████████| 1/1 [00:22<00:00, 22.26s/it]\u001b[0m\n",
      "\u001b[34mINFO:simpletransformers.classification.classification_model: Training of bert model complete. Saved to outputs/.\u001b[0m\n",
      "\u001b[34m2022-12-06 06:36:03,089 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-12-06 06:36:20 Uploading - Uploading generated training model\n",
      "2022-12-06 06:37:21 Completed - Training job completed\n",
      "Training seconds: 472\n",
      "Billable seconds: 472\n"
     ]
    }
   ],
   "source": [
    "pytorch_estimator1.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea4c4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_mnist_model_data=pytorch_estimator1.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28c1f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f4a1c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3Downloader.download(pt_mnist_model_data, \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84594e28",
   "metadata": {},
   "source": [
    "# Endpoint Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8b20125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "CUR = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "ENDPOINT_NAME = \"mme-hf-POC-V1\" + \"-\" + CUR\n",
    "\n",
    "MODEL_NAME = ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a188b981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mme-hf-POC-V1-2022-12-06-06-39-30'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55728ddc",
   "metadata": {},
   "source": [
    "# Create Model, Endpoint Config, Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7425849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model_uri=pytorch_estimator1.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb84e4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://agtimeseries/mce-folder/model2/bert-model-2022-12-06-06-28-18-002/output/model.tar.gz'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "db10cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_ecr_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version=\"1.9.0\",\n",
    "    py_version=\"py38\",\n",
    "    instance_type=\"ml.c5.4xlarge\",\n",
    "    image_scope=\"inference\",\n",
    "    container_version=\"ubuntu20.04\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64dd1cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-east-2.amazonaws.com/pytorch-inference:1.9.0-cpu-py38-ubuntu20.04'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_ecr_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc9b6079",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_container = {\n",
    "    \"ContainerHostname\": \"pytorch-model\",\n",
    "    \"Image\": pt_ecr_image_uri,\n",
    "    \"ModelDataUrl\": pt_model_uri,\n",
    "    \"Environment\": {\n",
    "        \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
    "        \"SAGEMAKER_SUBMIT_DIRECTORY\": pt_model_uri,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e695318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ContainerHostname': 'pytorch-model',\n",
       " 'Image': '763104351884.dkr.ecr.us-east-2.amazonaws.com/pytorch-inference:1.9.0-cpu-py38-ubuntu20.04',\n",
       " 'ModelDataUrl': 's3://agtimeseries/mce-folder/model2/bert-model-2022-12-06-06-28-18-002/output/model.tar.gz',\n",
       " 'Environment': {'SAGEMAKER_PROGRAM': 'inference.py',\n",
       "  'SAGEMAKER_SUBMIT_DIRECTORY': 's3://agtimeseries/mce-folder/model2/bert-model-2022-12-06-06-28-18-002/output/model.tar.gz'}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5bcc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "07a886ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_uri=huggingface_estimator1.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f8bc92ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client(\"sagemaker\")\n",
    "# image_uri will depend on the region\n",
    "image_uri = \"763104351884.dkr.ecr.us-east-2.amazonaws.com/huggingface-pytorch-inference:1.9.0-transformers4.11.0-cpu-py38-ubuntu20.04\"\n",
    "primary_container_hf = {\n",
    "    \"ContainerHostname\": \"huggingface-model\",\n",
    "    'Image': image_uri,\n",
    "    'ModelDataUrl': hf_model_uri,\n",
    "    'Environment': {\n",
    "        'SAGEMAKER_PROGRAM': 'inference.py',\n",
    "        'SAGEMAKER_REGION': region,\n",
    "        'SAGEMAKER_SUBMIT_DIRECTORY': hf_model_uri\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8dfc213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ContainerHostname': 'huggingface-model',\n",
       " 'Image': '763104351884.dkr.ecr.us-east-2.amazonaws.com/huggingface-pytorch-inference:1.9.0-transformers4.11.0-cpu-py38-ubuntu20.04',\n",
       " 'ModelDataUrl': 's3://agtimeseries/mce-hf-folder/model1/bert-model-2022-12-06-05-42-55-321/output/model.tar.gz',\n",
       " 'Environment': {'SAGEMAKER_PROGRAM': 'inference.py',\n",
       "  'SAGEMAKER_REGION': 'us-east-2',\n",
       "  'SAGEMAKER_SUBMIT_DIRECTORY': 's3://agtimeseries/mce-hf-folder/model1/bert-model-2022-12-06-05-42-55-321/output/model.tar.gz'}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primary_container_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d998cb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mme-hf-POC-V1-2022-12-06-06-39-30'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0375ce2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5ef7e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=ENDPOINT_NAME,\n",
    "    #multi-container-example\n",
    "    Containers=[pytorch_container, primary_container_hf],\n",
    "    InferenceExecutionConfig={\"Mode\": \"Direct\"},\n",
    "    ExecutionRoleArn=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "806419f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint configuration arn:  arn:aws:sagemaker:us-east-2:831536787935:endpoint-config/mme-hf-poc-v1-2022-12-06-06-39-30\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName =ENDPOINT_NAME,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "        'InstanceType':'ml.m5.4xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName': ENDPOINT_NAME,\n",
    "        'VariantName':'AllTraffic',\n",
    "        'InitialVariantWeight':1\n",
    "        }\n",
    "    ])\n",
    "print('Endpoint configuration arn:  {}'.format(endpoint_config_response['EndpointConfigArn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9828c8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndpointArn = arn:aws:sagemaker:us-east-2:831536787935:endpoint/mme-hf-poc-v1-2022-12-06-06-39-30\n"
     ]
    }
   ],
   "source": [
    "endpoint_params = {\n",
    "    'EndpointName': ENDPOINT_NAME,\n",
    "    'EndpointConfigName':ENDPOINT_NAME,\n",
    "}\n",
    "endpoint_response = sm_client.create_endpoint(**endpoint_params)\n",
    "print('EndpointArn = {}'.format(endpoint_response['EndpointArn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d75212ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "waiter=sm_client.get_waiter(\"endpoint_in_service\")\n",
    "waiter.wait(EndpointName=ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f16865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c213e527",
   "metadata": {},
   "source": [
    "# Endpoint Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9bc3c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen1='the universe mirrored in a puddleisnt it amazing how there always seems to be another bottle of bheer therealeph one bottles of beer on the wall aleph one null bottles of beer\\tyou too are a puddle\\tas above so below'\n",
    "sen2='the universe mirrored in a puddleisnt it amazing how there always seems to be another bottle of bheer therealeph one bottles of beer on the wall aleph one null bottles of beer\\tyou too are a puddle\\tas above so below'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "548393f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "v=[{\"text\":sen1,\"id\":1},{\"text\":sen2,\"id\":2}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "710c6d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "client = boto3.client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "614887dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"score\": 0.07771327346563339,\n",
      "    \"Categ\": 14,\n",
      "    \"pred\": \"i1\",\n",
      "    \"probabs\": {\n",
      "      \"0\": 0.062302861362695694,\n",
      "      \"1\": 0.02041207253932953,\n",
      "      \"2\": 0.06461289525032043,\n",
      "      \"3\": 0.03423217311501503,\n",
      "      \"4\": 0.044934581965208054,\n",
      "      \"5\": 0.024256065487861633,\n",
      "      \"6\": 0.036553822457790375,\n",
      "      \"7\": 0.06341090053319931,\n",
      "      \"8\": 0.07290441542863846,\n",
      "      \"9\": 0.031524669378995895,\n",
      "      \"10\": 0.03600924089550972,\n",
      "      \"11\": 0.04315823316574097,\n",
      "      \"12\": 0.038719259202480316,\n",
      "      \"13\": 0.05332612246274948,\n",
      "      \"14\": 0.07771327346563339,\n",
      "      \"15\": 0.07034662365913391,\n",
      "      \"16\": 0.029433589428663254,\n",
      "      \"17\": 0.0674159973859787,\n",
      "      \"18\": 0.06579737365245819,\n",
      "      \"19\": 0.06293588876724243\n",
      "    },\n",
      "    \"text\": \"the universe mirrored in a puddleisnt it amazing how there always seems to be another bottle of bheer therealeph one bottles of beer on the wall aleph one null bottles of beer\\tyou too are a puddle\\tas above so below\",\n",
      "    \"id\": 1\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.07771327346563339,\n",
      "    \"Categ\": 14,\n",
      "    \"pred\": \"i1\",\n",
      "    \"probabs\": {\n",
      "      \"0\": 0.062302861362695694,\n",
      "      \"1\": 0.02041207253932953,\n",
      "      \"2\": 0.06461289525032043,\n",
      "      \"3\": 0.03423217311501503,\n",
      "      \"4\": 0.044934581965208054,\n",
      "      \"5\": 0.024256065487861633,\n",
      "      \"6\": 0.036553822457790375,\n",
      "      \"7\": 0.06341090053319931,\n",
      "      \"8\": 0.07290441542863846,\n",
      "      \"9\": 0.031524669378995895,\n",
      "      \"10\": 0.03600924089550972,\n",
      "      \"11\": 0.04315823316574097,\n",
      "      \"12\": 0.038719259202480316,\n",
      "      \"13\": 0.05332612246274948,\n",
      "      \"14\": 0.07771327346563339,\n",
      "      \"15\": 0.07034662365913391,\n",
      "      \"16\": 0.029433589428663254,\n",
      "      \"17\": 0.0674159973859787,\n",
      "      \"18\": 0.06579737365245819,\n",
      "      \"19\": 0.06293588876724243\n",
      "    },\n",
      "    \"text\": \"the universe mirrored in a puddleisnt it amazing how there always seems to be another bottle of bheer therealeph one bottles of beer on the wall aleph one null bottles of beer\\tyou too are a puddle\\tas above so below\",\n",
      "    \"id\": 2\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "EndpointName=ENDPOINT_NAME,\n",
    "ContentType=\"application/json\",\n",
    "Accept=\"application/json\",\n",
    "TargetContainerHostname=\"huggingface-model\",\n",
    "Body=json.dumps(v)\n",
    ")\n",
    "print(response['Body'].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "47b0e83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"score\": \"0.09843205895225313\",\n",
      "    \"categ\": \"13\",\n",
      "    \"pred\": \"n\",\n",
      "    \"probabs\": {\n",
      "      \"0\": \"0.03503072614401855\",\n",
      "      \"1\": \"0.023843549854156376\",\n",
      "      \"2\": \"0.023444839068190566\",\n",
      "      \"3\": \"0.03545735012186487\",\n",
      "      \"4\": \"0.038312047150484814\",\n",
      "      \"5\": \"0.02553160807235324\",\n",
      "      \"6\": \"0.035870460194183586\",\n",
      "      \"7\": \"0.06402122147628944\",\n",
      "      \"8\": \"0.0908671579613727\",\n",
      "      \"9\": \"0.04136967595813908\",\n",
      "      \"10\": \"0.0731020017872005\",\n",
      "      \"11\": \"0.0515978588698538\",\n",
      "      \"12\": \"0.04358914971715622\",\n",
      "      \"13\": \"0.09843205895225313\",\n",
      "      \"14\": \"0.0694045428263539\",\n",
      "      \"15\": \"0.046193877917905725\",\n",
      "      \"16\": \"0.0269933735583257\",\n",
      "      \"17\": \"0.04977644705082381\",\n",
      "      \"18\": \"0.06078476619048829\",\n",
      "      \"19\": \"0.06637728712858572\"\n",
      "    },\n",
      "    \"text\": \"the universe mirrored in a puddleisnt it amazing how there always seems to be another bottle of bheer therealeph one bottles of beer on the wall aleph one null bottles of beer\\tyou too are a puddle\\tas above so below\",\n",
      "    \"id\": 1\n",
      "  },\n",
      "  {\n",
      "    \"score\": \"0.09843205895225313\",\n",
      "    \"categ\": \"13\",\n",
      "    \"pred\": \"n\",\n",
      "    \"probabs\": {\n",
      "      \"0\": \"0.03503072614401855\",\n",
      "      \"1\": \"0.023843549854156376\",\n",
      "      \"2\": \"0.023444839068190566\",\n",
      "      \"3\": \"0.03545735012186487\",\n",
      "      \"4\": \"0.038312047150484814\",\n",
      "      \"5\": \"0.02553160807235324\",\n",
      "      \"6\": \"0.035870460194183586\",\n",
      "      \"7\": \"0.06402122147628944\",\n",
      "      \"8\": \"0.0908671579613727\",\n",
      "      \"9\": \"0.04136967595813908\",\n",
      "      \"10\": \"0.0731020017872005\",\n",
      "      \"11\": \"0.0515978588698538\",\n",
      "      \"12\": \"0.04358914971715622\",\n",
      "      \"13\": \"0.09843205895225313\",\n",
      "      \"14\": \"0.0694045428263539\",\n",
      "      \"15\": \"0.046193877917905725\",\n",
      "      \"16\": \"0.0269933735583257\",\n",
      "      \"17\": \"0.04977644705082381\",\n",
      "      \"18\": \"0.06078476619048829\",\n",
      "      \"19\": \"0.06637728712858572\"\n",
      "    },\n",
      "    \"text\": \"the universe mirrored in a puddleisnt it amazing how there always seems to be another bottle of bheer therealeph one bottles of beer on the wall aleph one null bottles of beer\\tyou too are a puddle\\tas above so below\",\n",
      "    \"id\": 2\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "EndpointName=ENDPOINT_NAME,\n",
    "ContentType=\"application/json\",\n",
    "Accept=\"application/json\",\n",
    "TargetContainerHostname=\"pytorch-model\",\n",
    "Body=json.dumps(v)\n",
    ")\n",
    "print(response['Body'].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a8cbb1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"score\": 0.08863607794046402,\n",
      "    \"Categ\": 19,\n",
      "    \"pred\": \"n1\",\n",
      "    \"probabs\": {\n",
      "      \"0\": 0.0542377345263958,\n",
      "      \"1\": 0.04734285920858383,\n",
      "      \"2\": 0.02886742539703846,\n",
      "      \"3\": 0.03112703375518322,\n",
      "      \"4\": 0.02827124483883381,\n",
      "      \"5\": 0.02508690394461155,\n",
      "      \"6\": 0.022516116499900818,\n",
      "      \"7\": 0.0706554725766182,\n",
      "      \"8\": 0.07568445056676865,\n",
      "      \"9\": 0.05779770761728287,\n",
      "      \"10\": 0.04797745496034622,\n",
      "      \"11\": 0.06330781430006027,\n",
      "      \"12\": 0.042975474148988724,\n",
      "      \"13\": 0.057478226721286774,\n",
      "      \"14\": 0.06288282573223114,\n",
      "      \"15\": 0.05868130549788475,\n",
      "      \"16\": 0.060786861926317215,\n",
      "      \"17\": 0.03845611587166786,\n",
      "      \"18\": 0.03723089396953583,\n",
      "      \"19\": 0.08863607794046402\n",
      "    },\n",
      "    \"text\": \"the universe mirrored in a puddleisnt it amazing how there always seems to be another bottle of bheer therealeph one bottles of beer on the wall aleph one null bottles of beer\\tyou too are a puddle\\tas above so below\",\n",
      "    \"id\": 1\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.08863607794046402,\n",
      "    \"Categ\": 19,\n",
      "    \"pred\": \"n1\",\n",
      "    \"probabs\": {\n",
      "      \"0\": 0.0542377345263958,\n",
      "      \"1\": 0.04734285920858383,\n",
      "      \"2\": 0.02886742539703846,\n",
      "      \"3\": 0.03112703375518322,\n",
      "      \"4\": 0.02827124483883381,\n",
      "      \"5\": 0.02508690394461155,\n",
      "      \"6\": 0.022516116499900818,\n",
      "      \"7\": 0.0706554725766182,\n",
      "      \"8\": 0.07568445056676865,\n",
      "      \"9\": 0.05779770761728287,\n",
      "      \"10\": 0.04797745496034622,\n",
      "      \"11\": 0.06330781430006027,\n",
      "      \"12\": 0.042975474148988724,\n",
      "      \"13\": 0.057478226721286774,\n",
      "      \"14\": 0.06288282573223114,\n",
      "      \"15\": 0.05868130549788475,\n",
      "      \"16\": 0.060786861926317215,\n",
      "      \"17\": 0.03845611587166786,\n",
      "      \"18\": 0.03723089396953583,\n",
      "      \"19\": 0.08863607794046402\n",
      "    },\n",
      "    \"text\": \"the universe mirrored in a puddleisnt it amazing how there always seems to be another bottle of bheer therealeph one bottles of beer on the wall aleph one null bottles of beer\\tyou too are a puddle\\tas above so below\",\n",
      "    \"id\": 2\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "EndpointName=ENDPOINT_NAME,\n",
    "ContentType=\"application/json\",\n",
    "Accept=\"application/json\",\n",
    "TargetModel=\"model1.tar.gz\",\n",
    "Body=json.dumps(v)\n",
    ")\n",
    "print(response['Body'].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6a7d2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"score\": 0.08863607794046402,\n",
      "    \"Categ\": 19,\n",
      "    \"pred\": \"n1\",\n",
      "    \"probabs\": {\n",
      "      \"0\": 0.0542377345263958,\n",
      "      \"1\": 0.04734285920858383,\n",
      "      \"2\": 0.02886742539703846,\n",
      "      \"3\": 0.03112703375518322,\n",
      "      \"4\": 0.02827124483883381,\n",
      "      \"5\": 0.02508690394461155,\n",
      "      \"6\": 0.022516116499900818,\n",
      "      \"7\": 0.0706554725766182,\n",
      "      \"8\": 0.07568445056676865,\n",
      "      \"9\": 0.05779770761728287,\n",
      "      \"10\": 0.04797745496034622,\n",
      "      \"11\": 0.06330781430006027,\n",
      "      \"12\": 0.042975474148988724,\n",
      "      \"13\": 0.057478226721286774,\n",
      "      \"14\": 0.06288282573223114,\n",
      "      \"15\": 0.05868130549788475,\n",
      "      \"16\": 0.060786861926317215,\n",
      "      \"17\": 0.03845611587166786,\n",
      "      \"18\": 0.03723089396953583,\n",
      "      \"19\": 0.08863607794046402\n",
      "    },\n",
      "    \"text\": \"the universe mirrored in a puddleisnt it amazing how there always seems to be another bottle of bheer therealeph one bottles of beer on the wall aleph one null bottles of beer\\tyou too are a puddle\\tas above so below\",\n",
      "    \"id\": 1\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.08863607794046402,\n",
      "    \"Categ\": 19,\n",
      "    \"pred\": \"n1\",\n",
      "    \"probabs\": {\n",
      "      \"0\": 0.0542377345263958,\n",
      "      \"1\": 0.04734285920858383,\n",
      "      \"2\": 0.02886742539703846,\n",
      "      \"3\": 0.03112703375518322,\n",
      "      \"4\": 0.02827124483883381,\n",
      "      \"5\": 0.02508690394461155,\n",
      "      \"6\": 0.022516116499900818,\n",
      "      \"7\": 0.0706554725766182,\n",
      "      \"8\": 0.07568445056676865,\n",
      "      \"9\": 0.05779770761728287,\n",
      "      \"10\": 0.04797745496034622,\n",
      "      \"11\": 0.06330781430006027,\n",
      "      \"12\": 0.042975474148988724,\n",
      "      \"13\": 0.057478226721286774,\n",
      "      \"14\": 0.06288282573223114,\n",
      "      \"15\": 0.05868130549788475,\n",
      "      \"16\": 0.060786861926317215,\n",
      "      \"17\": 0.03845611587166786,\n",
      "      \"18\": 0.03723089396953583,\n",
      "      \"19\": 0.08863607794046402\n",
      "    },\n",
      "    \"text\": \"the universe mirrored in a puddleisnt it amazing how there always seems to be another bottle of bheer therealeph one bottles of beer on the wall aleph one null bottles of beer\\tyou too are a puddle\\tas above so below\",\n",
      "    \"id\": 2\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "EndpointName=ENDPOINT_NAME,\n",
    "ContentType=\"application/json\",\n",
    "Accept=\"application/json\",\n",
    "TargetModel=\"model1.tar.gz\",\n",
    "Body=json.dumps(v)\n",
    ")\n",
    "print(response['Body'].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2f5215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
